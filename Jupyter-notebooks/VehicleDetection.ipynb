{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2a98fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "676c32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the paths\n",
    "# custom\n",
    "# coco_names_path = '../Assets/yolo-coco/custom/classes.names'\n",
    "# weightsPath = '../Assets/yolo-coco/custom/Training-2/yolov3_custom_last_6000.weights'\n",
    "# configPath = '../Assets/yolo-coco/custom/yolov3_custom.cfg'\n",
    "\n",
    "# pretrained\n",
    "coco_names_path = '../Assets/yolo-coco/coco.names'\n",
    "weightsPath = '../Assets/yolo-coco/yolov3.weights'\n",
    "configPath = '../Assets/yolo-coco/yolov3.cfg'\n",
    "\n",
    "image_path = '../Assets/images/predict-2.jpeg'\n",
    "video_path = '../Assets/videos/vehicles1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab61c060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "\n",
    "Labels = []\n",
    "with open(coco_names_path,'r',encoding='utf8') as f:\n",
    "    Labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c917f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(Labels), 3),dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "413db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "# and determine only the *output* layer names that we need from YOLO\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(configPath,weightsPath)\n",
    "layers_names = net.getLayerNames()\n",
    "output_layers = [layers_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95423dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the video stream, pointer to output video file, and\n",
    "# frame dimensions\n",
    "vs = cv2.VideoCapture(image_path)\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "fps=vs.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f975804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1 total frames in video\n"
     ]
    }
   ],
   "source": [
    "# try to determine the total number of frames in the video file\n",
    "try:\n",
    "    prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \\\n",
    "        else cv2.CAP_PROP_FRAME_COUNT\n",
    "    total = int(vs.get(prop))\n",
    "    print(\"[INFO] {} total frames in video\".format(total))\n",
    "\n",
    "# an error occurred while trying to determine the total\n",
    "# number of frames in the video file\n",
    "except:\n",
    "    print(\"[INFO] could not determine # of frames in video\")\n",
    "    print(\"[INFO] no approx. completion time can be provided\")\n",
    "    total = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "197add06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vehicle_count(boxes, class_names):\n",
    "    \n",
    "    total_vehicle_count = 0 # total vechiles present in the image\n",
    "    dict_vehicle_count = {} # dictionary with count of each distinct vehicles detected\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "\n",
    "            class_name = class_names[i]\n",
    "       \n",
    "            if(class_name in list_of_vehicles):\n",
    "                total_vehicle_count += 1\n",
    "                dict_vehicle_count[class_name] = dict_vehicle_count.get(class_name,0) + 1\n",
    "\n",
    "    return total_vehicle_count, dict_vehicle_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b5612c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idxs -->  [ 40  14  66 132  22   8  37  45  27  46  82 108  18  12  96  68  80 126\n",
      "  85  76 130 137  93 106  56  53 118 103  60  74 136  62 122  89 141 105\n",
      " 100  58  51   4 144 139  87  88  50 127   2  57 143  52  81  70  61  91]\n",
      "init boxes--> [[572, 220, 75, 74], [389, 253, 86, 76], [525, 256, 81, 90], [688, 238, 94, 101], [699, 235, 95, 103], [524, 272, 85, 78], [804, 286, 112, 85], [786, 288, 140, 82], [934, 284, 94, 80], [940, 282, 91, 79], [335, 310, 108, 84], [322, 312, 130, 79], [343, 314, 105, 79], [575, 307, 100, 79], [801, 296, 117, 82], [796, 297, 122, 79], [336, 321, 107, 80], [345, 321, 101, 80], [568, 379, 127, 93], [565, 383, 133, 90], [581, 381, 120, 91], [1106, 384, 134, 102], [1100, 387, 142, 93], [1110, 387, 150, 92], [1103, 398, 138, 95], [1101, 400, 144, 92], [241, 446, 128, 96], [766, 430, 114, 107], [748, 435, 146, 97], [1001, 435, 152, 120], [1004, 433, 146, 123], [1005, 429, 142, 129], [1016, 437, 138, 114], [1005, 433, 160, 124], [1005, 432, 159, 124], [226, 455, 130, 106], [215, 458, 153, 102], [234, 455, 138, 108], [233, 458, 140, 102], [1001, 442, 153, 118], [1010, 442, 136, 117], [1003, 438, 146, 126], [1008, 437, 150, 127], [1007, 439, 153, 123], [1011, 438, 148, 130], [1081, 594, 74, 118], [470, 651, 186, 68], [1072, 641, 85, 80], [464, 665, 197, 58], [547, 24, 30, 20], [556, 23, 31, 17], [810, 25, 15, 24], [609, 38, 36, 24], [756, 35, 40, 29], [752, 33, 44, 35], [875, 32, 35, 31], [872, 27, 41, 38], [568, 51, 35, 23], [639, 48, 42, 23], [696, 45, 46, 32], [694, 44, 48, 36], [1060, 47, 54, 36], [533, 63, 36, 25], [530, 58, 44, 38], [927, 52, 61, 44], [925, 52, 66, 43], [929, 54, 70, 43], [931, 54, 66, 43], [1039, 55, 63, 41], [1034, 55, 73, 41], [531, 75, 36, 26], [526, 66, 44, 39], [637, 73, 50, 43], [638, 75, 54, 39], [582, 84, 59, 48], [592, 79, 47, 52], [639, 76, 45, 44], [639, 78, 51, 39], [709, 85, 53, 45], [782, 75, 56, 49], [787, 75, 54, 49], [868, 93, 18, 27], [507, 98, 55, 41], [494, 97, 70, 41], [587, 90, 49, 44], [711, 91, 49, 43], [711, 90, 55, 45], [864, 101, 21, 33], [867, 117, 19, 24], [457, 124, 63, 39], [454, 125, 67, 37], [883, 129, 24, 31], [878, 126, 30, 46], [958, 123, 30, 49], [453, 131, 67, 62], [557, 133, 63, 49], [557, 134, 73, 48], [560, 135, 67, 48], [709, 134, 54, 58], [712, 134, 53, 57], [881, 144, 24, 33], [879, 134, 32, 45], [959, 142, 25, 35], [1009, 136, 30, 50], [1113, 141, 23, 36], [1112, 137, 28, 46], [451, 148, 66, 46], [448, 150, 69, 43], [710, 142, 54, 52], [711, 142, 55, 54], [1012, 156, 24, 35], [1008, 146, 33, 47], [1113, 154, 26, 32], [524, 179, 75, 51], [525, 180, 73, 50], [531, 179, 73, 49], [411, 190, 71, 56], [410, 189, 73, 55], [522, 181, 79, 60], [526, 181, 73, 59], [529, 179, 75, 63], [527, 179, 79, 64], [409, 194, 74, 58], [410, 197, 69, 54], [567, 220, 75, 52], [569, 224, 78, 62], [566, 226, 77, 57], [399, 254, 70, 65], [1109, 260, 39, 58], [1050, 276, 38, 48], [1110, 266, 37, 62], [1111, 265, 43, 64], [1049, 280, 39, 63], [1112, 282, 34, 51], [1114, 285, 39, 50], [1051, 299, 38, 53], [576, 304, 97, 69], [1050, 312, 37, 49], [1119, 328, 42, 52], [1118, 340, 45, 47], [1228, 342, 50, 51], [1228, 346, 49, 69], [1237, 344, 39, 73], [1232, 382, 42, 55], [1091, 651, 55, 71]]\n",
      "init names--> ['car', 'car', 'car', 'car', 'car', 'truck', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'person', 'car', 'bicycle', 'car', 'car', 'car', 'person', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'person', 'car', 'car', 'car', 'car', 'car', 'person', 'motorbike', 'car', 'car', 'person', 'person', 'person', 'car', 'car', 'car', 'car', 'car', 'car', 'motorbike', 'motorbike', 'motorbike', 'person', 'person', 'person', 'car', 'car', 'car', 'car', 'motorbike', 'motorbike', 'motorbike', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'car', 'person', 'person', 'person', 'person', 'person', 'motorbike', 'motorbike', 'motorbike', 'car', 'motorbike', 'person', 'motorbike', 'person', 'person', 'person', 'motorbike', 'bicycle']\n",
      "Total vehicles in image 43\n",
      "Each vehicles count in image {'car': 37, 'motorbike': 5, 'bicycle': 1}\n",
      "[INFO] single frame took 0.2955 seconds\n",
      "[INFO] estimated total time to finish: 0.2955\n"
     ]
    }
   ],
   "source": [
    "# list_of_vehicles = ['car', 'truck', 'bus', 'motorbike', 'bicycle']\n",
    "list_of_vehicles = ['car', 'truck', 'bus', 'motorbike', 'bicycle', 'ambulance', 'fire engine', 'auto rickshaw']\n",
    "# looping over the frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end\n",
    "    # of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # if the frame dimensions are empty, grab them\n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes\n",
    "    # and associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    layerOutputs = net.forward(output_layers)\n",
    "    end = time.time()\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, confidences,\n",
    "    # and class IDs, respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    classname = []\n",
    "#     print(layerOutputs)\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability)\n",
    "            # of the current object detection\n",
    "            \n",
    "#             print(detection)\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > 0.4:\n",
    "                # scale the bounding box coordinates back relative to\n",
    "                # the size of the image, keeping in mind that YOLO\n",
    "                # actually returns the center (x, y)-coordinates of\n",
    "                # the bounding box followed by the boxes' width and\n",
    "                # height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top\n",
    "                # and and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates,\n",
    "                # confidences, and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "                classname.append(Labels[classID])\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    # bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.4,0.4)\n",
    "\n",
    "    \n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            # draw a bounding box rectangle and label on the frame\n",
    "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            \n",
    "            text = \"{}: {:.4f}\".format(Labels[classIDs[i]],confidences[i])\n",
    "            \n",
    "            cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    cv2.waitKey(0)\n",
    "    # check if the video writer is None\n",
    "    print(\"idxs --> \",idxs)\n",
    "    print(\"init boxes-->\",boxes)\n",
    "    print(\"init names-->\",classname)\n",
    "    \n",
    "    boxes = [boxes[i] for i in idxs]\n",
    "    classname = [classname[i] for i in idxs]\n",
    "    \n",
    "#     print(\"final boxes -->\",boxes)\n",
    "#     print(\"final names -->\",classname)\n",
    "    total_vehicles, each_vehicle = get_vehicle_count(boxes, classname)\n",
    "    print(\"Total vehicles in image\", total_vehicles)\n",
    "    print(\"Each vehicles count in image\", each_vehicle)\n",
    "    if writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter('../Assets/images/predictedResult.jpeg', fourcc, 30,\n",
    "            (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "        # some information on processing single frame\n",
    "        if total > 0:\n",
    "            elap = (end - start)\n",
    "            print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
    "            print(\"[INFO] estimated total time to finish: {:.4f}\".format(\n",
    "                elap * total))\n",
    "\n",
    "    # write the output frame to disk\n",
    "    writer.write(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f21f0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] cleaning up...\n"
     ]
    }
   ],
   "source": [
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "car=each_vehicle.get('car',0)\n",
    "motorbike=each_vehicle.get('motorbike',0)\n",
    "bicycle=each_vehicle.get('bicycle',0)\n",
    "bus=each_vehicle.get('bus',0)\n",
    "truck=each_vehicle.get('truck',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(car,motorbike,bicycle,bus,truck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8556b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer= (truck*3 + bus*3 + car*2 + motorbike*1 + bicycle*2.5)\n",
    "print(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cde17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the paths\n",
    "coco_names_path = '../Assets/yolo-coco/custom/classes.names'\n",
    "weightsPath = '../Assets/yolo-coco/custom/yolov3_custom_last_15600.weights'\n",
    "configPath = '../Assets/yolo-coco/custom/yolov3_custom.cfg'\n",
    "\n",
    "image_path = '../Assets/images/predict-3.jpeg'\n",
    "video_path = '../Assets/videos/vehicles1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bf13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "image = cv2.imread(image_path)\n",
    "classes = coco_names_path\n",
    "with open(classes, 'r') as f:\n",
    "    LABELS = [line.strip() for line in f.readlines()]\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "\tdtype=\"uint8\")\n",
    "print(\"[INFO]: Loading image...\")\n",
    "net = cv2.dnn.readNet(weightsPath,configPath)\n",
    "try:\n",
    "\t(H, W) = image.shape[:2]\n",
    "\tprint(\"[INFO]: Detecting object from image...\")\n",
    "except Exception as inst:\n",
    "\tprint('[INFO]: Got Error while loading image:',inst)\n",
    "\traise SystemExit()\n",
    "\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\tswapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "start = time.time()\n",
    "layerOutputs = net.forward(ln)\n",
    "end = time.time()\n",
    "print(\"[INFO]: YOLO took {:.6f} seconds\".format(end - start))\n",
    "boxes = []\n",
    "confidences = []\n",
    "classIDs = []\n",
    "for output in layerOutputs:\n",
    "\tfor detection in output:\n",
    "\t\tscores = detection[5:]\n",
    "\t\tclassID = np.argmax(scores)\n",
    "\t\tconfidence = scores[classID]\n",
    "\t\tif confidence > 0.4:\n",
    "\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\ty = int(centerY - (height / 2))\n",
    "\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\tconfidences.append(float(confidence))\n",
    "\t\t\tclassIDs.append(classID)\n",
    "idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.4,0.4)\n",
    "if len(idxs) > 0:\n",
    "\tfor i in idxs.flatten():\n",
    "\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t0.5, color, 2)\n",
    "cv2.imwrite('../Assets/images/predictedResult.jpeg', image)\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367f587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e47a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./keras-yolo3/yolo3/model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a68ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (D:\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13416/2601599633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_coordinator_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\engine\\keras_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# isort: off\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# Internal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# Deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\layer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\initializers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# isort: off\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\dtensor\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Conditional import the dtensor API, since it is currently broken in OSS.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_DTENSOR_API_ENABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdtensor_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Leave it with a placeholder, so that the import line from other python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (D:\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "import h5py\n",
    "import io\n",
    "# Load the pre-trained model\n",
    "model1 = yolo_body(Input(shape=(416, 416, 3)), 3, 80)\n",
    "\n",
    "# f = io.open('..', mode=\"rb\")\n",
    "    \n",
    "    \n",
    "model1.load_weights(\"D:\\Projects\\Course Projects\\MajorProject\\RushHour-Traffic-Signal-Models\\Assets\\yolo-coco\\yolov3.h5\", by_name=True)\n",
    "\n",
    "# Load the custom model\n",
    "model2 = load_model(\"D:\\Projects\\Course Projects\\MajorProject\\RushHour-Traffic-Signal-Models\\Assets\\yolo-coco\\custom\\yolov3_custom_last_11900.h5\")\n",
    "\n",
    "# Create an ensemble model\n",
    "ensemble_model = keras.models.Sequential()\n",
    "ensemble_model.add(keras.layers.average([model1, model2]))\n",
    "\n",
    "# Compile the ensemble model\n",
    "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Make predictions using the ensemble model\n",
    "image = load_image('../Assets/images/bus_cycle_car.jfif')\n",
    "predictions = ensemble_model.predict(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e94f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a5f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56525f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f46ce69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "\n",
    "def get_vehicle_count(boxes, class_names):\n",
    "    \n",
    "    list_of_vehicles = ['car', 'truck', 'bus', 'motorbike', 'bicycle']\n",
    "#     list_of_vehicles = ['car', 'truck', 'bus', 'motorbike', 'bicycle', 'ambulance', 'fire engine', 'auto rickshaw']\n",
    "    \n",
    "    total_vehicle_count = 0 # total vechiles present in the image\n",
    "    dict_vehicle_count = {} # dictionary with count of each distinct vehicles detected\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "\n",
    "            class_name = class_names[i]\n",
    "       \n",
    "            if(class_name in list_of_vehicles):\n",
    "                total_vehicle_count += 1\n",
    "                dict_vehicle_count[class_name] = dict_vehicle_count.get(class_name,0) + 1\n",
    "\n",
    "    return total_vehicle_count, dict_vehicle_count\n",
    "\n",
    "\n",
    "def detectVehicles():\n",
    "    \n",
    "    coco_names_path = '../Assets/yolo-coco/coco.names'\n",
    "    weightsPath = '../Assets/yolo-coco/yolov3.weights'\n",
    "    configPath = '../Assets/yolo-coco/yolov3.cfg'\n",
    "\n",
    "    image_path = '../Assets/images/predict-1.jpeg'\n",
    "    video_path = '../Assets/videos/vehicles1.mp4'\n",
    "    \n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "\n",
    "    Labels = []\n",
    "    with open(coco_names_path,'r',encoding='utf8') as f:\n",
    "        Labels = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        \n",
    "    # initialize a list of colors to represent each possible class label\n",
    "    np.random.seed(42)\n",
    "    COLORS = np.random.randint(0, 255, size=(len(Labels), 3),dtype=\"uint8\")\n",
    "    \n",
    "    # load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "    # and determine only the *output* layer names that we need from YOLO\n",
    "    net = cv2.dnn.readNetFromDarknet(configPath,weightsPath)\n",
    "    layers_names = net.getLayerNames()\n",
    "    output_layers = [layers_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    # initialize the video stream, pointer to output video file, and\n",
    "    # frame dimensions\n",
    "    vs = cv2.VideoCapture(image_path)\n",
    "    writer = None\n",
    "    (W, H) = (None, None)\n",
    "    fps=vs.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # try to determine the total number of frames in the video file\n",
    "    try:\n",
    "        prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() \\\n",
    "            else cv2.CAP_PROP_FRAME_COUNT\n",
    "        total = int(vs.get(prop))\n",
    "        print(\"[INFO] {} total frames in video\".format(total))\n",
    "\n",
    "    # an error occurred while trying to determine the total\n",
    "    # number of frames in the video file\n",
    "    except:\n",
    "        print(\"[INFO] could not determine # of frames in video\")\n",
    "        print(\"[INFO] no approx. completion time can be provided\")\n",
    "        total = -1\n",
    "    \n",
    "    # looping over the frames from the video file stream\n",
    "    while True:\n",
    "        # read the next frame from the file\n",
    "        (grabbed, frame) = vs.read()\n",
    "\n",
    "        # if the frame was not grabbed, then we have reached the end\n",
    "        # of the stream\n",
    "        if not grabbed:\n",
    "            break\n",
    "\n",
    "        # if the frame dimensions are empty, grab them\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "\n",
    "        # construct a blob from the input frame and then perform a forward\n",
    "        # pass of the YOLO object detector, giving us our bounding boxes\n",
    "        # and associated probabilities\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        start = time.time()\n",
    "        layerOutputs = net.forward(output_layers)\n",
    "        end = time.time()\n",
    "\n",
    "        # initialize our lists of detected bounding boxes, confidences,\n",
    "        # and class IDs, respectively\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        classIDs = []\n",
    "        classname = []\n",
    "    # print(layerOutputs)\n",
    "        # loop over each of the layer outputs\n",
    "        for output in layerOutputs:\n",
    "            # loop over each of the detections\n",
    "            for detection in output:\n",
    "                # extract the class ID and confidence (i.e., probability)\n",
    "                # of the current object detection\n",
    "\n",
    "    # print(detection)\n",
    "                scores = detection[5:]\n",
    "                classID = np.argmax(scores)\n",
    "                confidence = scores[classID]\n",
    "\n",
    "                # filter out weak predictions by ensuring the detected\n",
    "                # probability is greater than the minimum probability\n",
    "                if confidence > 0.5:\n",
    "                    # scale the bounding box coordinates back relative to\n",
    "                    # the size of the image, keeping in mind that YOLO\n",
    "                    # actually returns the center (x, y)-coordinates of\n",
    "                    # the bounding box followed by the boxes' width and\n",
    "                    # height\n",
    "                    box = detection[0:4] * np.array([W, H, W, H])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                    # use the center (x, y)-coordinates to derive the top\n",
    "                    # and and left corner of the bounding box\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    # update our list of bounding box coordinates,\n",
    "                    # confidences, and class IDs\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    classIDs.append(classID)\n",
    "                    classname.append(Labels[classID])\n",
    "\n",
    "        # apply non-maxima suppression to suppress weak, overlapping\n",
    "        # bounding boxes\n",
    "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5,0.5)\n",
    "\n",
    "\n",
    "        # ensure at least one detection exists\n",
    "        if len(idxs) > 0:\n",
    "            # loop over the indexes we are keeping\n",
    "            for i in idxs.flatten():\n",
    "                # extract the bounding box coordinates\n",
    "                (x, y) = (boxes[i][0], boxes[i][1])\n",
    "                (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "                # draw a bounding box rectangle and label on the frame\n",
    "                color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "                text = \"{}: {:.4f}\".format(Labels[classIDs[i]],confidences[i])\n",
    "\n",
    "                cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        cv2.imshow(\"frame\",frame)\n",
    "        cv2.waitKey(0)\n",
    "        # check if the video writer is None\n",
    "#         print(\"idxs --> \",idxs)\n",
    "#         print(\"init boxes-->\",boxes)\n",
    "#         print(\"init names-->\",classname)\n",
    "\n",
    "        boxes = [boxes[i] for i in idxs]\n",
    "        classname = [classname[i] for i in idxs]\n",
    "\n",
    "    #     print(\"final boxes -->\",boxes)\n",
    "    #     print(\"final names -->\",classname)\n",
    "        total_vehicles, each_vehicle = get_vehicle_count(boxes, classname)\n",
    "        print(\"Total vehicles in image\", total_vehicles)\n",
    "        print(\"Each vehicles count in image\", each_vehicle)\n",
    "        if writer is None:\n",
    "            # initialize our video writer\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "            writer = cv2.VideoWriter('../Assets/images/predictedResult.jpeg', fourcc, 30,\n",
    "                (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "            # some information on processing single frame\n",
    "            if total > 0:\n",
    "                elap = (end - start)\n",
    "                print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
    "                print(\"[INFO] estimated total time to finish: {:.4f}\".format(\n",
    "                    elap * total))\n",
    "\n",
    "        # write the output frame to disk\n",
    "        writer.write(frame)\n",
    "\n",
    "    # release the file pointers\n",
    "    print(\"[INFO] cleaning up...\")\n",
    "    writer.release()\n",
    "    vs.release()\n",
    "    \n",
    "    car=each_vehicle.get('car',0)\n",
    "    motorbike=each_vehicle.get('motorbike',0)\n",
    "    bicycle=each_vehicle.get('bicycle',0)\n",
    "    bus=each_vehicle.get('bus',0)\n",
    "    truck=each_vehicle.get('truck',0)\n",
    "    \n",
    "    \n",
    "    return each_vehicle\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da3c833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1 total frames in video\n",
      "Total vehicles in image 64\n",
      "Each vehicles count in image {'car': 58, 'truck': 5, 'bus': 1}\n",
      "[INFO] single frame took 0.5106 seconds\n",
      "[INFO] estimated total time to finish: 0.5106\n",
      "[INFO] cleaning up...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'car': 58, 'truck': 5, 'bus': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectVehicles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab277922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
